{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class LogisticRegression():\n",
    "\n",
    "\n",
    "    def __init__(self, num_features, learning_rate=0.01, regularization=None, C=1):\n",
    "        self.w = np.random.uniform(size=num_features)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_features = num_features\n",
    "        self.regularization = regularization\n",
    "        self.C = C\n",
    "\n",
    "    def _exp_dot(self, x):\n",
    "        return np.exp(x.dot(self.w))\n",
    "\n",
    "    def predict(self, x):\n",
    " \n",
    "        probs = sigmoid(self._exp_dot(x))\n",
    "        return (probs > 0.5).astype(np.int)\n",
    "\n",
    "    def gd(self, x, y):\n",
    "\n",
    "        probs = sigmoid(self._exp_dot(x))\n",
    "        gradients = (x.multiply((y - probs).reshape(-1, 1))).sum(0)\n",
    "        gradients = np.array(gradients.tolist()).reshape(self.num_features)\n",
    "        if self.regularization == \"l2\":\n",
    "            self.w += self.learning_rate * (gradients * self.C - self.w)\n",
    "        elif self.regularization == \"l1\":\n",
    "            self.w += self.learning_rate * (gradients * self.C - np.sign(self.w))\n",
    "        else:\n",
    "            self.w += self.learning_rate * gradients\n",
    "\n",
    "    def mle(self, x, y):\n",
    "        ''' Return the MLE estimates, log[p(y|x)]'''\n",
    "        return (y * x.dot(self.w) - np.log(1 + self._exp_dot(x))).sum()\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SoftmaxRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxRegression():\n",
    "\n",
    "\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01, regularization=None, C=1):\n",
    "        self.w = np.random.uniform(size=(num_features, num_classes))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.regularization = regularization\n",
    "        self.C = C\n",
    "\n",
    "    def predict(self, x):\n",
    "     \n",
    "        probs = softmax(x.dot(self.w))\n",
    "        return probs.argmax(-1)\n",
    "\n",
    "    def gd(self, x, y):\n",
    "\n",
    "        probs = softmax(x.dot(self.w))\n",
    "        gradients = x.transpose().dot(to_onehot(y, self.num_classes) - probs)\n",
    "        if self.regularization == \"l2\":\n",
    "            self.w += self.learning_rate * (gradients * self.C - self.w)\n",
    "        elif self.regularization == \"l1\":\n",
    "            self.w += self.learning_rate * (gradients * self.C - np.sign(self.w))\n",
    "        else:\n",
    "            self.w += self.learning_rate * gradients\n",
    "\n",
    "    def mle(self, x, y):\n",
    "\n",
    "        probs = softmax(x.dot(self.w))\n",
    "        return (to_onehot(y, self.num_classes) * np.log(probs)).sum()\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    return np.exp(x) / np.exp(x).sum(-1, keepdims=True)\n",
    "\n",
    "\n",
    "def to_onehot(x, class_num):\n",
    "    return np.eye(class_num)[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from LR_model import LogisticRegression\n",
    "from SR_model import SoftmaxRegression\n",
    "\n",
    "train_epochs = 10\n",
    "learning_rate = 0.00005\n",
    "batch_size = 1024\n",
    "class_num = 5\n",
    "data_path = \"data\"\n",
    "regularization = \"l1\"\n",
    "C = 0.8\n",
    "\n",
    "\n",
    "class Ngram():\n",
    "    def __init__(self, n_grams, max_tf=0.8):\n",
    "        ''' n_grams: tuple, n_gram range'''\n",
    "        self.n_grams = n_grams\n",
    "        self.tok2id = {}\n",
    "        self.tok2tf = Counter()\n",
    "        self.max_tf = max_tf\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        ''' In this task, we simply the following tokenizer.'''\n",
    "        return text.lower().split(\" \")\n",
    "\n",
    "    def get_n_grams(self, toks):\n",
    "        ngrams_toks = []\n",
    "        for ngrams in range(self.n_grams[0], self.n_grams[1] + 1):\n",
    "            for i in range(0, len(toks) - ngrams + 1):\n",
    "                ngrams_toks.append(' '.join(toks[i:i + ngrams]))\n",
    "        return ngrams_toks\n",
    "\n",
    "    def fit(self, datas, fix_vocab=False):\n",
    "        ''' Transform the data into n-gram vectors. Using csr_matrix to store this sparse matrix.'''\n",
    "        if not fix_vocab:\n",
    "            for data in datas:\n",
    "                toks = self.tokenize(data)\n",
    "                ngrams_toks = self.get_n_grams(toks)\n",
    "                self.tok2tf.update(Counter(ngrams_toks))\n",
    "            self.tok2tf = dict(filter(lambda x: x[1] < self.max_tf * len(datas), self.tok2tf.items()))\n",
    "            self.tok2id = dict([(k, i) for i, k in enumerate(self.tok2tf.keys())])\n",
    "        # the column indices for row i are stored in indices[indptr[i]:indptr[i+1]]\n",
    "        # and their corresponding values are stored in nums[indptr[i]:indptr[i+1]]\n",
    "        indices = []\n",
    "        indptr = [0]\n",
    "        nums = []\n",
    "        for data in datas:\n",
    "            toks = self.tokenize(data)\n",
    "            ngrams_counter = Counter(self.get_n_grams(toks))\n",
    "            for k, v in ngrams_counter.items():\n",
    "                if k in self.tok2id:\n",
    "                    indices.append(self.tok2id[k])\n",
    "                    nums.append(v)\n",
    "            indptr.append(len(indices))\n",
    "        return csr_matrix((nums, indices, indptr), dtype=int, shape=(len(datas), len(self.tok2id)))\n",
    "\n",
    "\n",
    "def train_test_split(X, Y, shuffle=True):\n",
    "    '''\n",
    "    Split data into train set, dev set and test set.\n",
    "    '''\n",
    "    assert X.shape[0] == Y.shape[0], \"The length of X and Y must be equal.\"\n",
    "    len_ = X.shape[0]\n",
    "    index = np.arange(0, len_)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(index)\n",
    "    train_num = int(0.8 * len_)\n",
    "    dev_num = int(0.1 * len_)\n",
    "    test_num = len_ - train_num - dev_num\n",
    "    return X[index[:train_num]], X[index[train_num:train_num + dev_num]], X[index[-test_num:]], \\\n",
    "           Y[index[:train_num]], Y[index[train_num:train_num + dev_num]], Y[index[-test_num:]]\n",
    "\n",
    "\n",
    "def minibatch(data, minibatch_idx):\n",
    "    return data[minibatch_idx] if type(data) in [np.ndarray, csr_matrix] else [data[i] for i in minibatch_idx]\n",
    "\n",
    "\n",
    "def get_minibatches(data, minibatch_size, shuffle=True):\n",
    "\n",
    "    list_data = type(data) is list and (type(data[0]) is list or type(data[0]) in [np.ndarray, csr_matrix])\n",
    "    if list_data:\n",
    "        data_size = data[0].shape[0] if type(data[0]) in [np.ndarray, csr_matrix] else len(data[0])\n",
    "    else:\n",
    "        data_size = data[0].shape[0] if type(data) in [np.ndarray, csr_matrix] else len(data)\n",
    "    indices = np.arange(data_size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    for minibatch_start in np.arange(0, data_size, minibatch_size):\n",
    "        minibatch_indices = indices[minibatch_start:minibatch_start + minibatch_size]\n",
    "        yield [minibatch(d, minibatch_indices) for d in data] if list_data \\\n",
    "            else minibatch(data, minibatch_indices)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    train = pd.read_csv(os.path.join(data_path, 'train.tsv'), sep='\\t')\n",
    "    test = pd.read_csv(os.path.join(data_path, 'test.tsv'), sep='\\t')\n",
    "\n",
    "    ngram = Ngram((1, 1))\n",
    "    X = ngram.fit(train['Phrase'])\n",
    "    Y = train['Sentiment'].values\n",
    "\n",
    "    # convert to 2 classes to test our LogisticRegression.\n",
    "    # Y = train['Sentiment'].apply(lambda x:1 if x>2 else 0).values\n",
    "    # lr = LogisticRegression(X.shape[1], learning_rate, \"l2\")\n",
    "\n",
    "    lr = SoftmaxRegression(X.shape[1], class_num, learning_rate, regularization, C)\n",
    "\n",
    "    train_X, dev_X, test_X, train_Y, dev_Y, test_Y = train_test_split(X, Y)\n",
    "\n",
    "    # # Method1: (batch) gradient descent\n",
    "    # for epoch in range(train_epochs):\n",
    "    #     train_mle = lr.mle(train_X, train_Y)\n",
    "    #     print(\"Epoch %s, Train MLE %.3f\" % (epoch, train_mle))\n",
    "    #     lr.gd(train_X, train_Y)\n",
    "    #     predict_dev_Y = lr.predict(dev_X)\n",
    "    #     print(\"Epoch %s, Dev Acc %.3f\" % (epoch, (predict_dev_Y == dev_Y).sum() / len(dev_Y)))\n",
    "\n",
    "    # # Method2: stochastic gradient descent\n",
    "    # for epoch in range(train_epochs):\n",
    "    #     for batch_X, batch_Y in get_minibatches([train_X, train_Y], 1, True):\n",
    "    #         lr.gd(batch_X, batch_Y)\n",
    "    #     predict_dev_Y = lr.predict(dev_X)\n",
    "    #     print(\"Epoch %s, Dev Acc %.3f\" % (epoch, (predict_dev_Y == dev_Y).sum() / len(dev_Y)))\n",
    "\n",
    "    # Method3: mini-batch gradient descent\n",
    "    for epoch in range(train_epochs):\n",
    "        for batch_X, batch_Y in get_minibatches([train_X, train_Y], batch_size, True):\n",
    "            lr.gd(batch_X, batch_Y)\n",
    "        predict_dev_Y = lr.predict(dev_X)\n",
    "        print(\"Epoch %s, Dev Acc %.3f\" % (epoch, (predict_dev_Y == dev_Y).sum() / len(dev_Y)))\n",
    "\n",
    "    # testing\n",
    "    predict_test_Y = lr.predict(test_X)\n",
    "    print(\"Test Acc %.3f\" % ((predict_test_Y == test_Y).sum() / len(test_Y)))\n",
    "\n",
    "    # predicting\n",
    "    to_predict_X = ngram.fit(test['Phrase'], fix_vocab=True)\n",
    "    test['Sentiment'] = lr.predict(to_predict_X)\n",
    "    test[['Sentiment', 'PhraseId']].set_index('PhraseId').to_csv('numpy_based_lr.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
