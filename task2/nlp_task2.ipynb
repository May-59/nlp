{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    " \n",
    "def data_split(data, test_rate=0.3):\n",
    "    \"\"\"把数据按一定比例划分成训练集和测试集\"\"\"\n",
    "    train = list()\n",
    "    test = list()\n",
    "    for datum in data:\n",
    "        if random.random() > test_rate:\n",
    "            train.append(datum)\n",
    "        else:\n",
    "            test.append(datum)\n",
    "    return train, test\n",
    " \n",
    " \n",
    "class Random_embedding():\n",
    "    \"\"\"随机初始化\"\"\"\n",
    "    def __init__(self, data, test_rate=0.3):\n",
    "        self.dict_words = dict()  # 单词->ID的映射\n",
    "        data.sort(key=lambda x:len(x[2].split()))  # 按照句子长度排序，短着在前，这样做可以避免后面一个batch内句子长短不一，导致padding过度\n",
    "        self.data = data\n",
    "        self.len_words = 0  # 单词数目（包括padding的ID：0）\n",
    "        self.train, self.test = data_split(data, test_rate=test_rate)  # 训练集测试集划分\n",
    "        self.train_y = [int(term[3]) for term in self.train]  # 训练集类别\n",
    "        self.test_y = [int(term[3]) for term in self.test]  # 测试集类别\n",
    "        self.train_matrix = list()  # 训练集的单词ID列表，叠成一个矩阵\n",
    "        self.test_matrix = list()  # 测试集的单词ID列表，叠成一个矩阵\n",
    "        self.longest=0  # 记录最长的单词\n",
    " \n",
    "    def get_words(self):\n",
    "        for term in self.data:\n",
    "            s = term[2]  # 取出句子\n",
    "            s = s.upper()  # 记得要全部转化为大写！！（或者全部小写，否则一个单词例如i，I会识别成不同的两个单词）\n",
    "            words = s.split()\n",
    "            for word in words:  # 一个一个单词寻找\n",
    "                if word not in self.dict_words:\n",
    "                    self.dict_words[word] = len(self.dict_words)+1  # padding是第0个，所以要+1\n",
    "        self.len_words=len(self.dict_words)  # 单词数目（暂未包括padding的ID：0）\n",
    " \n",
    "    def get_id(self):\n",
    "        for term in self.train:  # 训练集\n",
    "            s = term[2]\n",
    "            s = s.upper()\n",
    "            words = s.split()\n",
    "            item=[self.dict_words[word] for word in words]  # 找到id列表（未进行padding）\n",
    "            self.longest=max(self.longest,len(item))  # 记录最长的单词\n",
    "            self.train_matrix.append(item)\n",
    "        for term in self.test:\n",
    "            s = term[2]\n",
    "            s = s.upper()\n",
    "            words = s.split()\n",
    "            item = [self.dict_words[word] for word in words]  # 找到id列表（未进行padding）\n",
    "            self.longest = max(self.longest, len(item))  # 记录最长的单词\n",
    "            self.test_matrix.append(item)\n",
    "        self.len_words += 1   # 单词数目（包括padding的ID：0）\n",
    " \n",
    " \n",
    "class Glove_embedding():\n",
    "    def __init__(self, data,trained_dict,test_rate=0.3):\n",
    "        self.dict_words = dict()  # 单词->ID的映射\n",
    "        self.trained_dict=trained_dict  # 记录预训练词向量模型\n",
    "        data.sort(key=lambda x:len(x[2].split()))  # 按照句子长度排序，短着在前，这样做可以避免后面一个batch内句子长短不一，导致padding过度\n",
    "        self.data = data\n",
    "        self.len_words = 0  # 单词数目（包括padding的ID：0）\n",
    "        self.train, self.test = data_split(data, test_rate=test_rate)  # 训练集测试集划分\n",
    "        self.train_y = [int(term[3]) for term in self.train]  # 训练集类别\n",
    "        self.test_y = [int(term[3]) for term in self.test]  # 测试集类别\n",
    "        self.train_matrix = list()  # 训练集的单词ID列表，叠成一个矩阵\n",
    "        self.test_matrix = list()  # 测试集的单词ID列表，叠成一个矩阵\n",
    "        self.longest=0  # 记录最长的单词\n",
    "        self.embedding=list()  # 抽取出用到的（预训练模型的）单词\n",
    " \n",
    "    def get_words(self):\n",
    "        self.embedding.append([0] * 50)  # 先加padding的词向量\n",
    "        for term in self.data:\n",
    "            s = term[2]  # 取出句子\n",
    "            s = s.upper()  # 记得要全部转化为大写！！（或者全部小写，否则一个单词例如i，I会识别成不同的两个单词）\n",
    "            words = s.split()\n",
    "            for word in words:  # 一个一个单词寻找\n",
    "                if word not in self.dict_words:\n",
    "                    self.dict_words[word] = len(self.dict_words)+1  # padding是第0个，所以要+1\n",
    "                    if word in self.trained_dict:  # 如果预训练模型有这个单词，直接记录词向量\n",
    "                        self.embedding.append(self.trained_dict[word])\n",
    "                    else:  # 预训练模型没有这个单词，初始化该词对应的词向量为0向量\n",
    "                        # print(word)\n",
    "                        # raise Exception(\"words not found!\")\n",
    "                        self.embedding.append([0]*50)\n",
    "        self.len_words=len(self.dict_words)  # 单词数目（暂未包括padding的ID：0）\n",
    " \n",
    "    def get_id(self):\n",
    "        for term in self.train:  # 训练集\n",
    "            s = term[2]\n",
    "            s = s.upper()\n",
    "            words = s.split()\n",
    "            item=[self.dict_words[word] for word in words]  # 找到id列表（未进行padding）\n",
    "            self.longest=max(self.longest,len(item))  # 记录最长的单词\n",
    "            self.train_matrix.append(item)\n",
    "        for term in self.test:\n",
    "            s = term[2]\n",
    "            s = s.upper()\n",
    "            words = s.split()\n",
    "            item = [self.dict_words[word] for word in words]  # 找到id列表（未进行padding）\n",
    "            self.longest = max(self.longest, len(item))  # 记录最长的单词\n",
    "            self.test_matrix.append(item)\n",
    "        self.len_words += 1  # 单词数目（暂未包括padding的ID：0）\n",
    " \n",
    " \n",
    "class ClsDataset(Dataset):\n",
    "    \"\"\"自定义数据集的结构,pytroch基本功！！！\"\"\"\n",
    "    def __init__(self, sentence, emotion):\n",
    "        self.sentence = sentence  # 句子\n",
    "        self.emotion= emotion  # 情感类别\n",
    " \n",
    "    def __getitem__(self, item):\n",
    "        return self.sentence[item], self.emotion[item]\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.emotion)\n",
    " \n",
    " \n",
    "def collate_fn(batch_data):\n",
    "    \"\"\"自定义数据集的内数据返回方式,pytroch基本功！！！并进行padding！！！\"\"\"\n",
    "    sentence, emotion = zip(*batch_data)\n",
    "    sentences = [torch.LongTensor(sent) for sent in sentence]  # 把句子变成Longtensor类型\n",
    "    padded_sents = pad_sequence(sentences, batch_first=True, padding_value=0)  # 自动padding操作！！！\n",
    "    return torch.LongTensor(padded_sents), torch.LongTensor(emotion)\n",
    " \n",
    " \n",
    "def get_batch(x,y,batch_size):\n",
    "    \"\"\"利用dataloader划分batch,pytroch基本功！！！\"\"\"\n",
    "    dataset = ClsDataset(x, y)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,drop_last=True,collate_fn=collate_fn)\n",
    "    #  shuffle是指每个epoch都随机打乱数据排列再分batch，\n",
    "    #  这里一定要设置成false，否则之前的排序会直接被打乱，\n",
    "    #  drop_last是指不利用最后一个不完整的batch（数据大小不能被batch_size整除）\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    " \n",
    " \n",
    "class MY_RNN(nn.Module):\n",
    "    \"\"\"自己设计的RNN网络\"\"\"\n",
    "    def __init__(self, len_feature, len_hidden, len_words, typenum=5, weight=None, layer=1, nonlinearity='tanh',\n",
    "                 batch_first=True, drop_out=0.5):\n",
    "        super(MY_RNN, self).__init__()\n",
    "        self.len_feature = len_feature  # d的大小\n",
    "        self.len_hidden = len_hidden  # l_h的大小\n",
    "        self.len_words = len_words  # 单词的个数（包括padding）\n",
    "        self.layer = layer  # 隐藏层层数\n",
    "        self.dropout=nn.Dropout(drop_out)  # dropout层\n",
    "        if weight is None:  # 随机初始化\n",
    "            x = nn.init.xavier_normal_(torch.Tensor(len_words, len_feature))\n",
    "            self.embedding = nn.Embedding(num_embeddings=len_words, embedding_dim=len_feature, _weight=x).to(device)\n",
    "        else:  # GloVe初始化\n",
    "            self.embedding = nn.Embedding(num_embeddings=len_words, embedding_dim=len_feature, _weight=weight).to(device)\n",
    "        # 用nn.Module的内置函数定义隐藏层\n",
    "        self.rnn = nn.RNN(input_size=len_feature, hidden_size=len_hidden, num_layers=layer, nonlinearity=nonlinearity,\n",
    "                          batch_first=batch_first, dropout=drop_out).to(device)\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(len_hidden, typenum).to(device)\n",
    "        # 冗余的softmax层，可以不加\n",
    "        # self.act = nn.Softmax(dim=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        \"\"\"x:数据，维度为[batch_size， 句子长度]\"\"\"\n",
    "        x = torch.LongTensor(x).to(device)\n",
    "        batch_size = x.size(0)\n",
    "        \"\"\"经过词嵌入后，维度为[batch_size，句子长度，d]\"\"\"\n",
    "        out_put = self.embedding(x)  # 词嵌入\n",
    "        out_put=self.dropout(out_put)  # dropout层\n",
    "        \n",
    "        # 另一种初始化h_0的方式\n",
    "        # h0 = torch.randn(self.layer, batch_size, self.len_hidden).to(device)\n",
    "        # 初始化h_0为0向量\n",
    "        h0 = torch.autograd.Variable(torch.zeros(self.layer, batch_size, self.len_hidden)).to(device)\n",
    "        \"\"\"dropout后不变，经过隐藏层后，维度为[1，batch_size, l_h]\"\"\"\n",
    "        _, hn = self.rnn(out_put, h0)  # 隐藏层计算\n",
    "        \"\"\"经过全连接层后，维度为[1，batch_size, 5]\"\"\"\n",
    "        out_put = self.fc(hn).squeeze(0)  # 全连接层\n",
    "        \"\"\"挤掉第0维度，返回[batch_size, 5]的数据\"\"\"\n",
    "        # out_put = self.act(out_put)  # 冗余的softmax层，可以不加\n",
    "        return out_put\n",
    " \n",
    " \n",
    "class MY_CNN(nn.Module):\n",
    "    def __init__(self, len_feature, len_words, longest, typenum=5, weight=None,drop_out=0.5):\n",
    "        super(MY_CNN, self).__init__()\n",
    "        self.len_feature = len_feature  # d的大小\n",
    "        self.len_words = len_words  # 单词数目\n",
    "        self.longest = longest  # 最长句子单词书目\n",
    "        self.dropout = nn.Dropout(drop_out)  # Dropout层\n",
    "        if weight is None:  # 随机初始化\n",
    "            x = nn.init.xavier_normal_(torch.Tensor(len_words, len_feature))\n",
    "            self.embedding = nn.Embedding(num_embeddings=len_words, embedding_dim=len_feature, _weight=x).to(device)\n",
    "        else:  # GloVe初始化\n",
    "            self.embedding = nn.Embedding(num_embeddings=len_words, embedding_dim=len_feature, _weight=weight).to(device)\n",
    "         # Conv2d参数详解：（输入通道数：1，输出通道数：l_l，卷积核大小：（行数，列数））\n",
    "         # padding是指往句子两侧加 0，因为有的句子只有一个单词\n",
    "         # 那么 X 就是 1*50 对 W=2*50 的卷积核根本无法进行卷积操作\n",
    "         # 因此要在X两侧行加0（两侧列不加），（padding=（1，0））变成 3*50\n",
    "         # 又比如 padding=（2，0）变成 5*50\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, longest, (2, len_feature), padding=(1, 0)), nn.ReLU()).to(device)  # 第1个卷积核+激活层\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(1, longest, (3, len_feature), padding=(1, 0)), nn.ReLU()).to(device) # 第2个卷积核+激活层\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(1, longest, (4, len_feature), padding=(2, 0)), nn.ReLU()).to(device) # 第3个卷积核+激活层\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(1, longest, (5, len_feature), padding=(2, 0)), nn.ReLU()).to(device)# 第4个卷积核+激活层\n",
    "        # 全连接层\n",
    "        self.fc = nn.Linear(4 * longest, typenum).to(device)\n",
    "        # 冗余的softmax层，可以不加\n",
    "        # self.act = nn.Softmax(dim=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        \"\"\"x:数据，维度为[batch_size， 句子长度]\"\"\"\n",
    "        \n",
    "        x = torch.LongTensor(x).to(device)\n",
    "        \"\"\"经过词嵌入后，维度为[batch_size，1，句子长度，d]\"\"\"\n",
    "        out_put = self.embedding(x).view(x.shape[0], 1, x.shape[1], self.len_feature)  # 词嵌入\n",
    "        \"\"\"dropout后不变,记为X\"\"\"\n",
    "        out_put=self.dropout(out_put)  # dropout层\n",
    "        \n",
    "        \"\"\"X经过2*d卷积后，维度为[batch_size，l_l，句子长度+2-1，1]\"\"\"\n",
    "        \"\"\"挤掉第三维度（维度从0开始），[batch_size，l_l，句子长度+2-1]记为Y_1\"\"\"\n",
    "        \"\"\"注意：句子长度+2-1的2是padding造成的行数扩张\"\"\"\n",
    "        conv1 = self.conv1(out_put).squeeze(3)  # 第1个卷积\n",
    "        \n",
    "        \"\"\"X经过3*d卷积后，维度为[batch_size，l_l，句子长度+2-2，1]\"\"\"\n",
    "        \"\"\"挤掉第三维度（维度从0开始），[batch_size，l_l，句子长度+2-2]记为Y_2\"\"\"\n",
    "        conv2 = self.conv2(out_put).squeeze(3)  # 第2个卷积\n",
    "        \n",
    "        \"\"\"X经过4*d卷积后，维度为[batch_size，l_l，句子长度+4-3，1]\"\"\"\n",
    "        \"\"\"挤掉第三维度（维度从0开始），[batch_size，l_l，句子长度+4-3]记为Y_3\"\"\"\n",
    "        conv3 = self.conv3(out_put).squeeze(3)  # 第3个卷积\n",
    "        \n",
    "        \"\"\"X经过5*d卷积后，维度为[batch_size，l_l，句子长度+4-4，1]\"\"\"\n",
    "        \"\"\"挤掉第三维度（维度从0开始），[batch_size，l_l，句子长度+4-4]记为Y_4\"\"\"\n",
    "        conv4 = self.conv4(out_put).squeeze(3)  # 第4个卷积\n",
    "        \n",
    "        \"\"\"分别对（Y_1,Y_2,Y_3,Y_4）的第二维（维度从0开始）进行pooling\"\"\"\n",
    "        \"\"\"得到4个[batch_size,，l_l，1]的向量\"\"\"\n",
    "        pool1 = F.max_pool1d(conv1, conv1.shape[2])\n",
    "        pool2 = F.max_pool1d(conv2, conv2.shape[2])\n",
    "        pool3 = F.max_pool1d(conv3, conv3.shape[2])\n",
    "        pool4 = F.max_pool1d(conv4, conv4.shape[2])\n",
    "        \n",
    "        \"\"\"拼接得到[batch_size,，l_l*4，1]的向量\"\"\"\n",
    "        \"\"\"挤掉第二维（维度从0开始）为[batch_size,，l_l*4]\"\"\"\n",
    "        pool = torch.cat([pool1, pool2, pool3, pool4], 1).squeeze(2)  # 拼接起来\n",
    "        \"\"\"经过全连接层后，维度为[batch_size, 5]\"\"\"\n",
    "        out_put = self.fc(pool)  # 全连接层\n",
    "        # out_put = self.act(out_put)  # 冗余的softmax层，可以不加\n",
    "        return out_put"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "画图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "# from Neural_Network_batch import MY_RNN,MY_CNN\n",
    "# from feature_batch import get_batch\n",
    "device = ('cuda' if torch.cuda.is_available() else 'cpu') \n",
    " \n",
    "def NN_embdding(model, train,test, learning_rate, iter_times):\n",
    "    # 定义优化器（求参数）\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # 损失函数  \n",
    "    loss_fun = F.cross_entropy\n",
    "    # 损失值记录\n",
    "    train_loss_record=list()\n",
    "    test_loss_record=list()\n",
    "    long_loss_record=list()\n",
    "    # 准确率记录\n",
    "    train_record=list()\n",
    "    test_record=list()\n",
    "    long_record=list()\n",
    "    # torch.autograd.set_detect_anomaly(True)\n",
    "    # 训练阶段\n",
    "    for iteration in range(iter_times):\n",
    "        model.train()  # 重要！！！进入非训练模式\n",
    "        for i, batch in enumerate(train):\n",
    "            x, y = batch  # 取一个batch\n",
    "            y=y.to(device)\n",
    "            pred = model(x).to(device) # 计算输出\n",
    "            optimizer.zero_grad()  # 梯度初始化\n",
    "            loss = loss_fun(pred, y).to(device)  # 损失值计算\n",
    "            loss.backward()  # 反向传播梯度\n",
    "            optimizer.step()  # 更新参数\n",
    " \n",
    "        model.eval()  # 重要！！！进入非训练模式（测试模式）\n",
    "        # 本轮正确率记录\n",
    "        train_acc = list()\n",
    "        test_acc = list()\n",
    "        long_acc = list()\n",
    "        length = 20\n",
    "        # 本轮损失值记录\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        long_loss=0\n",
    "        for i, batch in enumerate(train):\n",
    "            x, y = batch  # 取一个batch\n",
    "            y=y.to(device)\n",
    "            pred = model(x).to(device)  # 计算输出\n",
    "            loss = loss_fun(pred, y).to(device)  # 损失值计算\n",
    "            train_loss += loss.item()  # 损失值累加\n",
    "            _, y_pre = torch.max(pred, -1)\n",
    "            # 计算本batch准确率\n",
    "            acc = torch.mean((torch.tensor(y_pre == y, dtype=torch.float)))\n",
    "            train_acc.append(acc)\n",
    " \n",
    "        for i, batch in enumerate(test):\n",
    "            x, y = batch  # 取一个batch\n",
    "            y=y.to(device)\n",
    "            pred = model(x).to(device)  # 计算输出\n",
    "            loss = loss_fun(pred, y).to(device)  # 损失值计算\n",
    "            test_loss += loss.item()  # 损失值累加\n",
    "            _, y_pre = torch.max(pred, -1)\n",
    "            # 计算本batch准确率\n",
    "            acc = torch.mean((torch.tensor(y_pre == y, dtype=torch.float)))\n",
    "            test_acc.append(acc)\n",
    "            if(len(x[0]))>length:  # 长句子侦测\n",
    "              long_acc.append(acc)\n",
    "              long_loss+=loss.item()\n",
    " \n",
    "        trains_acc = sum(train_acc) / len(train_acc)\n",
    "        tests_acc = sum(test_acc) / len(test_acc)\n",
    "        longs_acc = sum(long_acc) / len(long_acc)\n",
    " \n",
    "        train_loss_record.append(train_loss / len(train_acc))\n",
    "        test_loss_record.append(test_loss / len(test_acc))\n",
    "        long_loss_record.append(long_loss/len(long_acc))\n",
    "        train_record.append(trains_acc.cpu())\n",
    "        test_record.append(tests_acc.cpu())\n",
    "        long_record.append(longs_acc.cpu())\n",
    "        print(\"---------- Iteration\", iteration + 1, \"----------\")\n",
    "        print(\"Train loss:\", train_loss/ len(train_acc))\n",
    "        print(\"Test loss:\", test_loss/ len(test_acc))\n",
    "        print(\"Train accuracy:\", trains_acc)\n",
    "        print(\"Test accuracy:\", tests_acc)\n",
    "        print(\"Long sentence accuracy:\", longs_acc)\n",
    " \n",
    "    return train_loss_record,test_loss_record,long_loss_record,train_record,test_record,long_record\n",
    "\n",
    " \n",
    "def NN_embedding_plot(random_embedding,glove_embedding,learning_rate, batch_size, iter_times):\n",
    "    # 获得训练集和测试集的batch\n",
    "    train_random = get_batch(random_embedding.train_matrix,\n",
    "                             random_embedding.train_y, batch_size)\n",
    "    test_random = get_batch(random_embedding.test_matrix,\n",
    "                            random_embedding.test_y, batch_size)\n",
    "    train_glove = get_batch(glove_embedding.train_matrix,\n",
    "                            glove_embedding.train_y, batch_size)\n",
    "    test_glove = get_batch(random_embedding.test_matrix,\n",
    "                           glove_embedding.test_y, batch_size)\n",
    "    # 模型建立             \n",
    "    torch.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    random_rnn = MY_RNN(50, 50, random_embedding.len_words)\n",
    "    torch.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    random_cnn = MY_CNN(50, random_embedding.len_words, random_embedding.longest)\n",
    "    torch.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    glove_rnn = MY_RNN(50, 50, glove_embedding.len_words, weight=torch.tensor(glove_embedding.embedding, dtype=torch.float))\n",
    "    torch.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    glove_cnn = MY_CNN(50, glove_embedding.len_words, glove_embedding.longest,weight=torch.tensor(glove_embedding.embedding, dtype=torch.float))\n",
    "    # rnn+random\n",
    "    torch.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    trl_ran_rnn,tel_ran_rnn,lol_ran_rnn,tra_ran_rnn,tes_ran_rnn,lon_ran_rnn=\\\n",
    "        NN_embdding(random_rnn,train_random,test_random,learning_rate,  iter_times)\n",
    "    # cnn+random\n",
    "    torch.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    trl_ran_cnn,tel_ran_cnn,lol_ran_cnn, tra_ran_cnn, tes_ran_cnn, lon_ran_cnn = \\\n",
    "        NN_embdding(random_cnn, train_random,test_random, learning_rate, iter_times)\n",
    "    # rnn+glove\n",
    "    torch.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    trl_glo_rnn,tel_glo_rnn,lol_glo_rnn, tra_glo_rnn, tes_glo_rnn, lon_glo_rnn = \\\n",
    "        NN_embdding(glove_rnn, train_glove,test_glove, learning_rate, iter_times)\n",
    "    # cnn+glove\n",
    "    torch.manual_seed(2021)\n",
    "    torch.manual_seed(2021)\n",
    "    trl_glo_cnn,tel_glo_cnn,lol_glo_cnn, tra_glo_cnn, tes_glo_cnn, lon_glo_cnn= \\\n",
    "        NN_embdding(glove_cnn,train_glove,test_glove, learning_rate, iter_times)\n",
    "       # 画图部分 \n",
    "    x=list(range(1,iter_times+1))\n",
    "    matplotlib.pyplot.subplot(2, 2, 1)\n",
    "    matplotlib.pyplot.plot(x, trl_ran_rnn, 'r--', label='RNN+random')\n",
    "    matplotlib.pyplot.plot(x, trl_ran_cnn, 'g--', label='CNN+random')\n",
    "    matplotlib.pyplot.plot(x, trl_glo_rnn, 'b--', label='RNN+glove')\n",
    "    matplotlib.pyplot.plot(x, trl_glo_cnn, 'y--', label='CNN+glove')\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Train Loss\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Loss\")\n",
    "    matplotlib.pyplot.subplot(2, 2, 2)\n",
    "    matplotlib.pyplot.plot(x, tel_ran_rnn, 'r--', label='RNN+random')\n",
    "    matplotlib.pyplot.plot(x, tel_ran_cnn, 'g--', label='CNN+random')\n",
    "    matplotlib.pyplot.plot(x, tel_glo_rnn, 'b--', label='RNN+glove')\n",
    "    matplotlib.pyplot.plot(x, tel_glo_cnn, 'y--', label='CNN+glove')\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Test Loss\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Loss\")\n",
    "    matplotlib.pyplot.subplot(2, 2, 3)\n",
    "    matplotlib.pyplot.plot(x, tra_ran_rnn, 'r--', label='RNN+random')\n",
    "    matplotlib.pyplot.plot(x, tra_ran_cnn, 'g--', label='CNN+random')\n",
    "    matplotlib.pyplot.plot(x, tra_glo_rnn, 'b--', label='RNN+glove')\n",
    "    matplotlib.pyplot.plot(x, tra_glo_cnn, 'y--', label='CNN+glove')\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Train Accuracy\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Accuracy\")\n",
    "    matplotlib.pyplot.ylim(0, 1)\n",
    "    matplotlib.pyplot.subplot(2, 2, 4)\n",
    "    matplotlib.pyplot.plot(x, tes_ran_rnn, 'r--', label='RNN+random')\n",
    "    matplotlib.pyplot.plot(x, tes_ran_cnn, 'g--', label='CNN+random')\n",
    "    matplotlib.pyplot.plot(x, tes_glo_rnn, 'b--', label='RNN+glove')\n",
    "    matplotlib.pyplot.plot(x, tes_glo_cnn, 'y--', label='CNN+glove')\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Test Accuracy\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Accuracy\")\n",
    "    matplotlib.pyplot.ylim(0, 1)\n",
    "    matplotlib.pyplot.tight_layout()\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(8, 8, forward=True)\n",
    "    matplotlib.pyplot.savefig('main_plot.jpg')\n",
    "    matplotlib.pyplot.show()\n",
    "    matplotlib.pyplot.subplot(2, 1, 1)\n",
    "    matplotlib.pyplot.plot(x, lon_ran_rnn, 'r--', label='RNN+random')\n",
    "    matplotlib.pyplot.plot(x, lon_ran_cnn, 'g--', label='CNN+random')\n",
    "    matplotlib.pyplot.plot(x, lon_glo_rnn, 'b--', label='RNN+glove')\n",
    "    matplotlib.pyplot.plot(x, lon_glo_cnn, 'y--', label='CNN+glove')\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Long Sentence Accuracy\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Accuracy\")\n",
    "    matplotlib.pyplot.ylim(0, 1)\n",
    "    matplotlib.pyplot.subplot(2, 1, 2)\n",
    "    matplotlib.pyplot.plot(x, lol_ran_rnn, 'r--', label='RNN+random')\n",
    "    matplotlib.pyplot.plot(x, lol_ran_cnn, 'g--', label='CNN+random')\n",
    "    matplotlib.pyplot.plot(x, lol_glo_rnn, 'b--', label='RNN+glove')\n",
    "    matplotlib.pyplot.plot(x, lol_glo_cnn, 'y--', label='CNN+glove')\n",
    "    matplotlib.pyplot.legend()\n",
    "    matplotlib.pyplot.legend(fontsize=10)\n",
    "    matplotlib.pyplot.title(\"Long Sentence Loss\")\n",
    "    matplotlib.pyplot.xlabel(\"Iterations\")\n",
    "    matplotlib.pyplot.ylabel(\"Loss\")\n",
    "    matplotlib.pyplot.tight_layout()\n",
    "    fig = matplotlib.pyplot.gcf()\n",
    "    fig.set_size_inches(8, 8, forward=True)\n",
    "    matplotlib.pyplot.savefig('sub_plot.jpg')\n",
    "    matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp/ipykernel_9764/2653304791.py:53: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc = torch.mean((torch.tensor(y_pre == y, dtype=torch.float)))\n",
      "C:\\Users\\asus\\AppData\\Local\\Temp/ipykernel_9764/2653304791.py:64: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  acc = torch.mean((torch.tensor(y_pre == y, dtype=torch.float)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Iteration 1 ----------\n",
      "Train loss: 1.2256712754932018\n",
      "Test loss: 1.2463221549987793\n",
      "Train accuracy: tensor(0.5615)\n",
      "Test accuracy: tensor(0.5367)\n",
      "Long sentence accuracy: tensor(0.3700)\n",
      "---------- Iteration 2 ----------\n",
      "Train loss: 0.9195707147821374\n",
      "Test loss: 0.9813626293213137\n",
      "Train accuracy: tensor(0.6539)\n",
      "Test accuracy: tensor(0.6171)\n",
      "Long sentence accuracy: tensor(0.4800)\n",
      "---------- Iteration 3 ----------\n",
      "Train loss: 0.8435465441931278\n",
      "Test loss: 0.9398379614276271\n",
      "Train accuracy: tensor(0.6713)\n",
      "Test accuracy: tensor(0.6175)\n",
      "Long sentence accuracy: tensor(0.4873)\n",
      "---------- Iteration 4 ----------\n",
      "Train loss: 0.8127612660237409\n",
      "Test loss: 0.9353967193634279\n",
      "Train accuracy: tensor(0.6787)\n",
      "Test accuracy: tensor(0.6165)\n",
      "Long sentence accuracy: tensor(0.5153)\n",
      "---------- Iteration 5 ----------\n",
      "Train loss: 0.7794699674352593\n",
      "Test loss: 0.929412156023005\n",
      "Train accuracy: tensor(0.6902)\n",
      "Test accuracy: tensor(0.6200)\n",
      "Long sentence accuracy: tensor(0.4900)\n",
      "---------- Iteration 6 ----------\n",
      "Train loss: 0.7013032884772764\n",
      "Test loss: 0.8858653044187894\n",
      "Train accuracy: tensor(0.7176)\n",
      "Test accuracy: tensor(0.6376)\n",
      "Long sentence accuracy: tensor(0.5123)\n",
      "---------- Iteration 7 ----------\n",
      "Train loss: 0.6871529265281258\n",
      "Test loss: 0.9010981481562379\n",
      "Train accuracy: tensor(0.7191)\n",
      "Test accuracy: tensor(0.6312)\n",
      "Long sentence accuracy: tensor(0.5080)\n",
      "---------- Iteration 8 ----------\n",
      "Train loss: 0.6397290073950356\n",
      "Test loss: 0.8740570449060009\n",
      "Train accuracy: tensor(0.7399)\n",
      "Test accuracy: tensor(0.6487)\n",
      "Long sentence accuracy: tensor(0.5280)\n",
      "---------- Iteration 9 ----------\n",
      "Train loss: 0.6363484032383753\n",
      "Test loss: 0.8892970001825722\n",
      "Train accuracy: tensor(0.7384)\n",
      "Test accuracy: tensor(0.6421)\n",
      "Long sentence accuracy: tensor(0.5143)\n",
      "---------- Iteration 10 ----------\n",
      "Train loss: 0.6185302091847866\n",
      "Test loss: 0.8948068477774179\n",
      "Train accuracy: tensor(0.7479)\n",
      "Test accuracy: tensor(0.6387)\n",
      "Long sentence accuracy: tensor(0.5117)\n",
      "---------- Iteration 11 ----------\n",
      "Train loss: 0.6248307262948893\n",
      "Test loss: 0.9244879894359137\n",
      "Train accuracy: tensor(0.7417)\n",
      "Test accuracy: tensor(0.6380)\n",
      "Long sentence accuracy: tensor(0.4990)\n",
      "---------- Iteration 12 ----------\n",
      "Train loss: 0.5994721219763843\n",
      "Test loss: 0.9210221568743387\n",
      "Train accuracy: tensor(0.7552)\n",
      "Test accuracy: tensor(0.6385)\n",
      "Long sentence accuracy: tensor(0.5163)\n",
      "---------- Iteration 13 ----------\n",
      "Train loss: 0.5906670871404333\n",
      "Test loss: 0.9075106138824135\n",
      "Train accuracy: tensor(0.7598)\n",
      "Test accuracy: tensor(0.6475)\n",
      "Long sentence accuracy: tensor(0.5283)\n",
      "---------- Iteration 14 ----------\n",
      "Train loss: 0.5785732758017855\n",
      "Test loss: 0.9212350101881129\n",
      "Train accuracy: tensor(0.7643)\n",
      "Test accuracy: tensor(0.6419)\n",
      "Long sentence accuracy: tensor(0.5163)\n",
      "---------- Iteration 15 ----------\n",
      "Train loss: 0.5750131316551375\n",
      "Test loss: 0.9347992565042229\n",
      "Train accuracy: tensor(0.7636)\n",
      "Test accuracy: tensor(0.6392)\n",
      "Long sentence accuracy: tensor(0.5023)\n",
      "---------- Iteration 16 ----------\n",
      "Train loss: 0.5687787633286704\n",
      "Test loss: 0.9354464700145106\n",
      "Train accuracy: tensor(0.7658)\n",
      "Test accuracy: tensor(0.6378)\n",
      "Long sentence accuracy: tensor(0.5167)\n",
      "---------- Iteration 17 ----------\n",
      "Train loss: 0.5913855334350823\n",
      "Test loss: 0.9923442140702279\n",
      "Train accuracy: tensor(0.7526)\n",
      "Test accuracy: tensor(0.6267)\n",
      "Long sentence accuracy: tensor(0.5147)\n",
      "---------- Iteration 18 ----------\n",
      "Train loss: 0.562861898965245\n",
      "Test loss: 0.9689990000058246\n",
      "Train accuracy: tensor(0.7657)\n",
      "Test accuracy: tensor(0.6331)\n",
      "Long sentence accuracy: tensor(0.5143)\n",
      "---------- Iteration 19 ----------\n",
      "Train loss: 0.548415199366458\n",
      "Test loss: 0.9643949750931032\n",
      "Train accuracy: tensor(0.7717)\n",
      "Test accuracy: tensor(0.6371)\n",
      "Long sentence accuracy: tensor(0.5170)\n",
      "---------- Iteration 20 ----------\n",
      "Train loss: 0.5529693523678211\n",
      "Test loss: 0.9894116476017941\n",
      "Train accuracy: tensor(0.7683)\n",
      "Test accuracy: tensor(0.6345)\n",
      "Long sentence accuracy: tensor(0.5153)\n",
      "---------- Iteration 21 ----------\n",
      "Train loss: 0.5341143630308296\n",
      "Test loss: 0.9818107671635126\n",
      "Train accuracy: tensor(0.7774)\n",
      "Test accuracy: tensor(0.6392)\n",
      "Long sentence accuracy: tensor(0.5227)\n",
      "---------- Iteration 22 ----------\n",
      "Train loss: 0.5309124079729439\n",
      "Test loss: 0.9855321581645679\n",
      "Train accuracy: tensor(0.7794)\n",
      "Test accuracy: tensor(0.6430)\n",
      "Long sentence accuracy: tensor(0.5290)\n",
      "---------- Iteration 23 ----------\n",
      "Train loss: 0.5278509375083884\n",
      "Test loss: 0.99957626353028\n",
      "Train accuracy: tensor(0.7810)\n",
      "Test accuracy: tensor(0.6415)\n",
      "Long sentence accuracy: tensor(0.5190)\n",
      "---------- Iteration 24 ----------\n",
      "Train loss: 0.5360504591478668\n",
      "Test loss: 1.0145543550932279\n",
      "Train accuracy: tensor(0.7761)\n",
      "Test accuracy: tensor(0.6335)\n",
      "Long sentence accuracy: tensor(0.5250)\n",
      "---------- Iteration 25 ----------\n",
      "Train loss: 0.5196560763437813\n",
      "Test loss: 1.0011245460920437\n",
      "Train accuracy: tensor(0.7841)\n",
      "Test accuracy: tensor(0.6374)\n",
      "Long sentence accuracy: tensor(0.5120)\n",
      "---------- Iteration 26 ----------\n",
      "Train loss: 0.5131624314011237\n",
      "Test loss: 1.0115378082439463\n",
      "Train accuracy: tensor(0.7881)\n",
      "Test accuracy: tensor(0.6351)\n",
      "Long sentence accuracy: tensor(0.5290)\n",
      "---------- Iteration 27 ----------\n",
      "Train loss: 0.534530380962912\n",
      "Test loss: 1.041173138926106\n",
      "Train accuracy: tensor(0.7767)\n",
      "Test accuracy: tensor(0.6291)\n",
      "Long sentence accuracy: tensor(0.5067)\n",
      "---------- Iteration 28 ----------\n",
      "Train loss: 0.5129739113083673\n",
      "Test loss: 1.029755804487454\n",
      "Train accuracy: tensor(0.7885)\n",
      "Test accuracy: tensor(0.6363)\n",
      "Long sentence accuracy: tensor(0.5107)\n",
      "---------- Iteration 29 ----------\n",
      "Train loss: 0.5522033014086956\n",
      "Test loss: 1.0737572049581876\n",
      "Train accuracy: tensor(0.7694)\n",
      "Test accuracy: tensor(0.6244)\n",
      "Long sentence accuracy: tensor(0.4983)\n",
      "---------- Iteration 30 ----------\n",
      "Train loss: 0.5619508538292636\n",
      "Test loss: 1.1040148529955136\n",
      "Train accuracy: tensor(0.7671)\n",
      "Test accuracy: tensor(0.6169)\n",
      "Long sentence accuracy: tensor(0.5037)\n",
      "---------- Iteration 31 ----------\n",
      "Train loss: 0.6432081220688624\n",
      "Test loss: 1.1939447137617296\n",
      "Train accuracy: tensor(0.7423)\n",
      "Test accuracy: tensor(0.5984)\n",
      "Long sentence accuracy: tensor(0.4517)\n",
      "---------- Iteration 32 ----------\n",
      "Train loss: 0.553162025099774\n",
      "Test loss: 1.0984392749365939\n",
      "Train accuracy: tensor(0.7713)\n",
      "Test accuracy: tensor(0.6188)\n",
      "Long sentence accuracy: tensor(0.4867)\n",
      "---------- Iteration 33 ----------\n",
      "Train loss: 0.5047110032392751\n",
      "Test loss: 1.0579041351554215\n",
      "Train accuracy: tensor(0.7896)\n",
      "Test accuracy: tensor(0.6292)\n",
      "Long sentence accuracy: tensor(0.5087)\n",
      "---------- Iteration 34 ----------\n",
      "Train loss: 0.5234014283899867\n",
      "Test loss: 1.0942334186646245\n",
      "Train accuracy: tensor(0.7815)\n",
      "Test accuracy: tensor(0.6237)\n",
      "Long sentence accuracy: tensor(0.4930)\n",
      "---------- Iteration 35 ----------\n",
      "Train loss: 0.4925244086242597\n",
      "Test loss: 1.072247495574336\n",
      "Train accuracy: tensor(0.7958)\n",
      "Test accuracy: tensor(0.6256)\n",
      "Long sentence accuracy: tensor(0.5067)\n",
      "---------- Iteration 36 ----------\n",
      "Train loss: 0.5184917949276779\n",
      "Test loss: 1.1071744240740293\n",
      "Train accuracy: tensor(0.7822)\n",
      "Test accuracy: tensor(0.6183)\n",
      "Long sentence accuracy: tensor(0.4843)\n",
      "---------- Iteration 37 ----------\n",
      "Train loss: 0.5157497864421627\n",
      "Test loss: 1.1179455287994877\n",
      "Train accuracy: tensor(0.7854)\n",
      "Test accuracy: tensor(0.6206)\n",
      "Long sentence accuracy: tensor(0.4963)\n",
      "---------- Iteration 38 ----------\n",
      "Train loss: 0.4915326895927071\n",
      "Test loss: 1.1020530667356265\n",
      "Train accuracy: tensor(0.7951)\n",
      "Test accuracy: tensor(0.6268)\n",
      "Long sentence accuracy: tensor(0.4983)\n",
      "---------- Iteration 39 ----------\n",
      "Train loss: 0.48392890453543685\n",
      "Test loss: 1.0866983475223664\n",
      "Train accuracy: tensor(0.7982)\n",
      "Test accuracy: tensor(0.6251)\n",
      "Long sentence accuracy: tensor(0.5067)\n",
      "---------- Iteration 40 ----------\n",
      "Train loss: 0.49216919353043814\n",
      "Test loss: 1.113566001256307\n",
      "Train accuracy: tensor(0.7952)\n",
      "Test accuracy: tensor(0.6239)\n",
      "Long sentence accuracy: tensor(0.4823)\n",
      "---------- Iteration 41 ----------\n",
      "Train loss: 0.45885730838967026\n",
      "Test loss: 1.079389734934735\n",
      "Train accuracy: tensor(0.8095)\n",
      "Test accuracy: tensor(0.6316)\n",
      "Long sentence accuracy: tensor(0.5207)\n",
      "---------- Iteration 42 ----------\n",
      "Train loss: 0.4792173179635488\n",
      "Test loss: 1.1262963183464543\n",
      "Train accuracy: tensor(0.8003)\n",
      "Test accuracy: tensor(0.6271)\n",
      "Long sentence accuracy: tensor(0.4970)\n",
      "---------- Iteration 43 ----------\n",
      "Train loss: 0.4727048720788518\n",
      "Test loss: 1.117544816386315\n",
      "Train accuracy: tensor(0.8013)\n",
      "Test accuracy: tensor(0.6256)\n",
      "Long sentence accuracy: tensor(0.4947)\n",
      "---------- Iteration 44 ----------\n",
      "Train loss: 0.46127160447179727\n",
      "Test loss: 1.1228774830859194\n",
      "Train accuracy: tensor(0.8080)\n",
      "Test accuracy: tensor(0.6286)\n",
      "Long sentence accuracy: tensor(0.4953)\n",
      "---------- Iteration 45 ----------\n",
      "Train loss: 0.44760039177031147\n",
      "Test loss: 1.1175504166592833\n",
      "Train accuracy: tensor(0.8134)\n",
      "Test accuracy: tensor(0.6341)\n",
      "Long sentence accuracy: tensor(0.5087)\n",
      "---------- Iteration 46 ----------\n",
      "Train loss: 0.46331172610778326\n",
      "Test loss: 1.1489716306809457\n",
      "Train accuracy: tensor(0.8080)\n",
      "Test accuracy: tensor(0.6250)\n",
      "Long sentence accuracy: tensor(0.4950)\n",
      "---------- Iteration 47 ----------\n",
      "Train loss: 0.4601388335262143\n",
      "Test loss: 1.1383144317134735\n",
      "Train accuracy: tensor(0.8074)\n",
      "Test accuracy: tensor(0.6256)\n",
      "Long sentence accuracy: tensor(0.5007)\n",
      "---------- Iteration 48 ----------\n",
      "Train loss: 0.4308813939708362\n",
      "Test loss: 1.126601222381797\n",
      "Train accuracy: tensor(0.8217)\n",
      "Test accuracy: tensor(0.6322)\n",
      "Long sentence accuracy: tensor(0.5130)\n",
      "---------- Iteration 49 ----------\n",
      "Train loss: 0.43014502882478983\n",
      "Test loss: 1.1408765290373115\n",
      "Train accuracy: tensor(0.8218)\n",
      "Test accuracy: tensor(0.6325)\n",
      "Long sentence accuracy: tensor(0.5050)\n",
      "---------- Iteration 50 ----------\n",
      "Train loss: 0.4342234035731729\n",
      "Test loss: 1.1426014650252558\n",
      "Train accuracy: tensor(0.8197)\n",
      "Test accuracy: tensor(0.6285)\n",
      "Long sentence accuracy: tensor(0.5110)\n",
      "---------- Iteration 1 ----------\n",
      "Train loss: 1.3066283967516839\n",
      "Test loss: 1.3357257061107184\n",
      "Train accuracy: tensor(0.5212)\n",
      "Test accuracy: tensor(0.4960)\n",
      "Long sentence accuracy: tensor(0.4017)\n",
      "---------- Iteration 2 ----------\n",
      "Train loss: 1.1064637735349323\n",
      "Test loss: 1.1895421538301694\n",
      "Train accuracy: tensor(0.5372)\n",
      "Test accuracy: tensor(0.4974)\n",
      "Long sentence accuracy: tensor(0.4973)\n",
      "---------- Iteration 3 ----------\n",
      "Train loss: 1.0428615351882549\n",
      "Test loss: 1.1698497610707437\n",
      "Train accuracy: tensor(0.5562)\n",
      "Test accuracy: tensor(0.4997)\n",
      "Long sentence accuracy: tensor(0.5007)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9764/3071993439.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mglove_embedding\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 找到每个句子拥有的单词ID\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mNN_embedding_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_embedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mglove_embedding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miter_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9764/2653304791.py\u001b[0m in \u001b[0;36mNN_embedding_plot\u001b[1;34m(random_embedding, glove_embedding, learning_rate, batch_size, iter_times)\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[0mtrl_ran_cnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtel_ran_cnn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlol_ran_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtra_ran_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtes_ran_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlon_ran_cnn\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mNN_embdding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_cnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_random\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_random\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miter_times\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[1;31m# rnn+glove\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2021\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9764/2653304791.py\u001b[0m in \u001b[0;36mNN_embdding\u001b[1;34m(model, train, test, learning_rate, iter_times)\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m  \u001b[1;31m# 取一个batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 计算输出\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 损失值计算\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 损失值累加\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1560\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_9764/1242092521.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpool1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpool4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 拼接起来\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;34m\"\"\"经过全连接层后，维度为[batch_size, 5]\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0mout_put\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 全连接层\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m         \u001b[1;31m# out_put = self.act(out_put)  # 冗余的softmax层，可以不加\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout_put\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1560\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1563\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1564\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\anaconda\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import random\n",
    "# from feature_batch import Random_embedding,Glove_embedding\n",
    "import torch\n",
    "# from comparison_plot_batch import NN_embedding_plot\n",
    " \n",
    "# 数据读入\n",
    "with open('train.tsv') as f:\n",
    "    tsvreader = csv.reader (f, delimiter ='\\t')\n",
    "    temp = list ( tsvreader )\n",
    " \n",
    "with open('glove.6B.50d.txt','rb') as f:  # for glove embedding\n",
    "    lines=f.readlines()\n",
    " \n",
    "# 用GloVe创建词典\n",
    "trained_dict=dict()\n",
    "n=len(lines)\n",
    "for i in range(n):\n",
    "    line=lines[i].split()\n",
    "    trained_dict[line[0].decode(\"utf-8\").upper()]=[float(line[j]) for j in range(1,51)]\n",
    " \n",
    "# 初始化\n",
    "iter_times=50  # 做50个epoch\n",
    "alpha=0.001\n",
    " \n",
    "# 程序开始\n",
    "data = temp[1:]\n",
    "batch_size=500\n",
    " \n",
    "# 随机初始化\n",
    "random.seed(2021)\n",
    "random_embedding=Random_embedding(data=data)\n",
    "random_embedding.get_words()  # 找到所有单词，并标记ID\n",
    "random_embedding.get_id()  # 找到每个句子拥有的单词ID\n",
    " \n",
    "# 预训练模型初始化\n",
    "random.seed(2021)\n",
    "glove_embedding=Glove_embedding(data=data,trained_dict=trained_dict)\n",
    "glove_embedding.get_words()  # 找到所有单词，并标记ID\n",
    "glove_embedding.get_id()  # 找到每个句子拥有的单词ID\n",
    " \n",
    "NN_embedding_plot(random_embedding,glove_embedding,alpha,batch_size,iter_times)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
